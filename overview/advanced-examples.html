
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="lang:clipboard.copy" content="Copy to clipboard">
  <meta name="lang:clipboard.copied" content="Copied to clipboard">
  <meta name="lang:search.language" content="en">
  <meta name="lang:search.pipeline.stopwords" content="True">
  <meta name="lang:search.pipeline.trimmer" content="True">
  <meta name="lang:search.result.none" content="No matching documents">
  <meta name="lang:search.result.one" content="1 matching document">
  <meta name="lang:search.result.other" content="# matching documents">
  <meta name="lang:search.tokenizer" content="[\s\-]+">

  
    <link href="https://fonts.gstatic.com/" rel="preconnect" crossorigin>
    <link href="https://fonts.googleapis.com/css?family=Roboto+Mono:400,500,700|Roboto:300,400,400i,700&display=fallback" rel="stylesheet">

    <style>
      body,
      input {
        font-family: "Roboto", "Helvetica Neue", Helvetica, Arial, sans-serif
      }

      code,
      kbd,
      pre {
        font-family: "Roboto Mono", "Courier New", Courier, monospace
      }
    </style>
  

  <link rel="stylesheet" href="../_static/stylesheets/application.css"/>
  <link rel="stylesheet" href="../_static/stylesheets/application-palette.css"/>
  <link rel="stylesheet" href="../_static/stylesheets/application-fixes.css"/>
  
  <link rel="stylesheet" href="../_static/fonts/material-icons.css"/>
  
  <meta name="theme-color" content="#3f51b5">
  <script src="../_static/javascripts/modernizr.js"></script>
  
  
  
    <title>Examples &#8212; Vulkan Kompute 0.5.0 documentation</title>
    <link rel="stylesheet" href="../_static/material.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Python Package Overview" href="python-package.html" />
    <link rel="prev" title="Kompute Docs Home" href="../index.html" />
  
   

  </head>
  <body dir=ltr
        data-md-color-primary=red data-md-color-accent=light-blue>
  
  <svg class="md-svg">
    <defs data-children-count="0">
      
      <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448" viewBox="0 0 416 448" id="__github"><path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg>
      
    </defs>
  </svg>
  
  <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer">
  <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search">
  <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
  <a href="#overview/advanced-examples" tabindex="1" class="md-skip"> Skip to content </a>
  <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex navheader">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="../index.html" title="Vulkan Kompute 0.5.0 documentation"
           class="md-header-nav__button md-logo">
          
            &nbsp;
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          <span class="md-header-nav__topic">Vulkan Kompute</span>
          <span class="md-header-nav__topic"> Examples </span>
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
        
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" action="../search.html" method="GET" name="search">
      <input type="text" class="md-search__input" name="q" placeholder="Search"
             autocapitalize="off" autocomplete="off" spellcheck="false"
             data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>

      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            <a href="https://github.com/EthicalML/vulkan-kompute/" title="Go to repository" class="md-source" data-md-source="github">

    <div class="md-source__icon">
      <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 24 24" width="28" height="28">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    Vulkan Kompute
  </div>
</a>
          </div>
        </div>
      
      
  
  <script src="../_static/javascripts/version_dropdown.js"></script>
  <script>
    var json_loc = "../"versions.json"",
        target_loc = "../../",
        text = "Versions";
    $( document ).ready( add_version_dropdown(json_loc, target_loc, text));
  </script>
  

    </div>
  </nav>
</header>

  
  <div class="md-container">
    
    
    
  <nav class="md-tabs" data-md-component="tabs">
    <div class="md-tabs__inner md-grid">
      <ul class="md-tabs__list">
          <li class="md-tabs__item"><a href="../index.html" class="md-tabs__link">Vulkan Kompute 0.5.0 documentation</a></li>
      </ul>
    </div>
  </nav>
    <main class="md-main">
      <div class="md-main__inner md-grid" data-md-component="container">
        
          <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
            <div class="md-sidebar__scrollwrap">
              <div class="md-sidebar__inner">
                <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="../index.html" title="Vulkan Kompute 0.5.0 documentation" class="md-nav__button md-logo">
      
        <img src="../_static/" alt=" logo" width="48" height="48">
      
    </a>
    <a href="../index.html"
       title="Vulkan Kompute 0.5.0 documentation">Vulkan Kompute</a>
  </label>
    <div class="md-nav__source">
      <a href="https://github.com/EthicalML/vulkan-kompute/" title="Go to repository" class="md-source" data-md-source="github">

    <div class="md-source__icon">
      <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 24 24" width="28" height="28">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    Vulkan Kompute
  </div>
</a>
    </div>
  
  

  
  <ul class="md-nav__list">
    <li class="md-nav__item">
    
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    <label class="md-nav__link md-nav__link--active" for="__toc"> Simple & Advanced Examples </label>
    
      <a href="#" class="md-nav__link md-nav__link--active">Simple & Advanced Examples</a>
      
        
<nav class="md-nav md-nav--secondary">
  <ul class="md-nav__list" data-md-scrollfix="">
    
<li class="md-nav__item"><a class="md-nav__extra_link" href="../_sources/overview/advanced-examples.rst.txt">Show Source</a> </li>

  </ul>
</nav>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="python-package.html" class="md-nav__link">Python Package Overview</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="async-parallel.html" class="md-nav__link">Asynchronous & Parallel Operations</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="memory-management.html" class="md-nav__link">Memory Management Principles</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="build-system.html" class="md-nav__link">Build System Deep Dive</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="shaders-to-headers.html" class="md-nav__link">Converting GLSL/HLSL Shaders to C++ Headers</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="mobile-android.html" class="md-nav__link">Mobile App Integration (Android)</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="game-engine-godot.html" class="md-nav__link">Game Engine Integration (Godot Engine)</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="python-reference.html" class="md-nav__link">Python Class Documentation & Reference</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="reference.html" class="md-nav__link">C++ Class Documentation & Reference</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../genindex.html" class="md-nav__link">Code Index</a>
      
    
    </li>
  </ul>
  

</nav>
              </div>
            </div>
          </div>
          <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
            <div class="md-sidebar__scrollwrap">
              <div class="md-sidebar__inner">
                
<nav class="md-nav md-nav--secondary">
  <ul class="md-nav__list" data-md-scrollfix="">
    
<li class="md-nav__item"><a class="md-nav__extra_link" href="../_sources/overview/advanced-examples.rst.txt">Show Source</a> </li>

<li id="searchbox" class="md-nav__item"></li>

  </ul>
</nav>
              </div>
            </div>
          </div>
        
        <div class="md-content">
          <article class="md-content__inner md-typeset" role="main">
            
  
<h1 id="overview-advanced-examples--page-root">Examples<a class="headerlink" href="#overview-advanced-examples--page-root" title="Permalink to this headline">¶</a></h1>
<p>The power of Kompute comes in when the interface is used for complex computations. This section contains an outline of the advanced / end-to-end examples available.</p>

<h2 id="simple-examples">Simple examples<a class="headerlink" href="#simple-examples" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="#simple-shader-example">Pass shader as raw string</a></p></li>
<li><p><a class="reference external" href="#record-batch-commands">Record batch commands with a Kompute Sequence</a></p></li>
<li><p><a class="reference external" href="#asynchronous-operations">Run Asynchronous Operations</a></p></li>
<li><p><a class="reference external" href="#parallel-operations">Run Parallel Operations Across Multiple GPU Queues</a></p></li>
<li><p><a class="reference external" href="#your-custom-kompute-operation">Create your custom Kompute Operations</a></p></li>
<li><p><a class="reference external" href="#logistic-regression-example">Implementing logistic regression from scratch</a></p></li>
</ul>


<h2 id="end-to-end-examples">End-to-end examples<a class="headerlink" href="#end-to-end-examples" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://towardsdatascience.com/machine-learning-and-data-processing-in-the-gpu-with-vulkan-kompute-c9350e5e5d3a">Machine Learning Logistic Regression Implementation</a></p></li>
<li><p><a class="reference external" href="https://towardsdatascience.com/parallelizing-heavy-gpu-workloads-via-multi-queue-operations-50a38b15a1dc">Parallelizing GPU-intensive Workloads via Multi-Queue Operations</a></p></li>
<li><p><a class="reference external" href="https://towardsdatascience.com/gpu-accelerated-machine-learning-in-your-mobile-applications-using-the-android-ndk-vulkan-kompute-1e9da37b7617">Android NDK Mobile Kompute ML Application</a></p></li>
<li><p><a class="reference external" href="https://towardsdatascience.com/supercharging-game-development-with-gpu-accelerated-ml-using-vulkan-kompute-the-godot-game-engine-4e75a84ea9f0">Game Development Kompute ML in Godot Engine</a></p></li>
</ul>

<h3 id="simple-shader-example">Simple Shader Example<a class="headerlink" href="#simple-shader-example" title="Permalink to this headline">¶</a></h3>
<p>Pass compute shader data in glsl/hlsl text or compiled SPIR-V format (or as path to the file). Back to <a class="reference external" href="#simple-examples">examples list</a>.</p>


<h3 id="record-batch-commands">Record batch commands<a class="headerlink" href="#record-batch-commands" title="Permalink to this headline">¶</a></h3>
<p>Record commands in a single submit by using a Sequence to send in batch to GPU. Back to <a class="reference external" href="#simple-examples">examples list</a></p>
<div class="highlight-cpp notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="kt">int</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>

    <span class="n">kp</span><span class="o">::</span><span class="n">Manager</span> <span class="n">mgr</span><span class="p">;</span>

    <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">tensorLHS</span><span class="p">{</span> <span class="k">new</span> <span class="n">kp</span><span class="o">::</span><span class="n">Tensor</span><span class="p">({</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span> <span class="p">})</span> <span class="p">};</span>
    <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">tensorRHS</span><span class="p">{</span> <span class="k">new</span> <span class="n">kp</span><span class="o">::</span><span class="n">Tensor</span><span class="p">({</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span> <span class="p">})</span> <span class="p">};</span>
    <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">tensorOutput</span><span class="p">{</span> <span class="k">new</span> <span class="n">kp</span><span class="o">::</span><span class="n">Tensor</span><span class="p">({</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span> <span class="p">})</span> <span class="p">};</span>

    <span class="c1">// Create all the tensors in memory</span>
    <span class="n">mgr</span><span class="p">.</span><span class="n">evalOpDefault</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">OpCreateTensor</span><span class="o">&gt;</span><span class="p">({</span><span class="n">tensorLHS</span><span class="p">,</span> <span class="n">tensorRHS</span><span class="p">,</span> <span class="n">tensorOutput</span><span class="p">});</span>

    <span class="c1">// Create a new sequence</span>
    <span class="n">std</span><span class="o">::</span><span class="n">weak_ptr</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">Sequence</span><span class="o">&gt;</span> <span class="n">sqWeakPtr</span> <span class="o">=</span> <span class="n">mgr</span><span class="p">.</span><span class="n">getOrCreateManagedSequence</span><span class="p">();</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">Sequence</span><span class="o">&gt;</span> <span class="n">sq</span> <span class="o">=</span> <span class="n">sqWeakPtr</span><span class="p">.</span><span class="n">lock</span><span class="p">())</span>
    <span class="p">{</span>
        <span class="c1">// Begin recording commands</span>
        <span class="n">sq</span><span class="o">-&gt;</span><span class="n">begin</span><span class="p">();</span>

        <span class="c1">// Record batch commands to send to GPU</span>
        <span class="n">sq</span><span class="o">-&gt;</span><span class="n">record</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">OpMult</span><span class="o">&lt;&gt;&gt;</span><span class="p">({</span> <span class="n">tensorLHS</span><span class="p">,</span> <span class="n">tensorRHS</span><span class="p">,</span> <span class="n">tensorOutput</span> <span class="p">});</span>
        <span class="n">sq</span><span class="o">-&gt;</span><span class="n">record</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">OpTensorCopy</span><span class="o">&gt;</span><span class="p">({</span><span class="n">tensorOutput</span><span class="p">,</span> <span class="n">tensorLHS</span><span class="p">,</span> <span class="n">tensorRHS</span><span class="p">});</span>

        <span class="c1">// Stop recording</span>
        <span class="n">sq</span><span class="o">-&gt;</span><span class="n">end</span><span class="p">();</span>

        <span class="c1">// Submit multiple batch operations to GPU</span>
        <span class="kt">size_t</span> <span class="n">ITERATIONS</span> <span class="o">=</span> <span class="mi">5</span><span class="p">;</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">ITERATIONS</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">sq</span><span class="o">-&gt;</span><span class="n">eval</span><span class="p">();</span>
        <span class="p">}</span>

        <span class="c1">// Sync GPU memory back to local tensor</span>
        <span class="n">sq</span><span class="o">-&gt;</span><span class="n">begin</span><span class="p">();</span>
        <span class="n">sq</span><span class="o">-&gt;</span><span class="n">record</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">OpTensorSyncLocal</span><span class="o">&gt;</span><span class="p">({</span><span class="n">tensorOutput</span><span class="p">});</span>
        <span class="n">sq</span><span class="o">-&gt;</span><span class="n">end</span><span class="p">();</span>
        <span class="n">sq</span><span class="o">-&gt;</span><span class="n">eval</span><span class="p">();</span>
    <span class="p">}</span>

    <span class="c1">// Print the output which iterates through OpMult 5 times</span>
    <span class="c1">// in this case the output is {32, 32 , 32}</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">fmt</span><span class="o">::</span><span class="n">format</span><span class="p">(</span><span class="s">"Output: {}"</span><span class="p">,</span> <span class="n">tensorOutput</span><span class="p">.</span><span class="n">data</span><span class="p">())</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</td></tr></table></div>


<h3 id="asynchronous-operations">Asynchronous Operations<a class="headerlink" href="#asynchronous-operations" title="Permalink to this headline">¶</a></h3>
<p>You can submit operations asynchronously with the async/await commands in the kp::Manager and kp::Sequence, which provides granularity on waiting on the vk::Fence. Back to <a class="reference external" href="#simple-examples">examples list</a></p>
<div class="highlight-cpp notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="kt">int</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>

    <span class="c1">// You can allow Kompute to create the Vulkan components, or pass your existing ones</span>
    <span class="n">kp</span><span class="o">::</span><span class="n">Manager</span> <span class="n">mgr</span><span class="p">;</span> <span class="c1">// Selects device 0 unless explicitly requested</span>

    <span class="c1">// Creates tensor an initializes GPU memory (below we show more granularity)</span>
    <span class="k">auto</span> <span class="n">tensor</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="p">(</span><span class="n">kp</span><span class="o">::</span><span class="n">Tensor</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)));</span>

    <span class="c1">// Create tensors data explicitly in GPU with an operation</span>
    <span class="n">mgr</span><span class="p">.</span><span class="n">evalOpAsyncDefault</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">OpTensorCreate</span><span class="o">&gt;</span><span class="p">({</span> <span class="n">tensor</span> <span class="p">});</span>

    <span class="c1">// Define your shader as a string (using string literals for simplicity)</span>
    <span class="c1">// (You can also pass the raw compiled bytes, or even path to file)</span>
    <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">shader</span><span class="p">(</span><span class="sa">R</span><span class="s">"</span><span class="dl">(</span><span class="s"></span>
<span class="s">        #version 450</span>

<span class="s">        layout (local_size_x = 1) in;</span>

<span class="s">        layout(set = 0, binding = 0) buffer b { float pb[]; };</span>

<span class="s">        shared uint sharedTotal[1];</span>

<span class="s">        void main() {</span>
<span class="s">            uint index = gl_GlobalInvocationID.x;</span>

<span class="s">            sharedTotal[0] = 0;</span>

<span class="s">            // Iterating to simulate longer process</span>
<span class="s">            for (int i = 0; i &lt; 100000000; i++)</span>
<span class="s">            {</span>
<span class="s">                atomicAdd(sharedTotal[0], 1);</span>
<span class="s">            }</span>

<span class="s">            pb[index] = sharedTotal[0];</span>
<span class="s">        }</span>
<span class="s">    </span><span class="dl">)</span><span class="s">"</span><span class="p">);</span>

    <span class="c1">// We can now await for the previous submitted command</span>
    <span class="c1">// The first parameter can be the amount of time to wait</span>
    <span class="c1">// The time provided is in nanoseconds</span>
    <span class="n">mgr</span><span class="p">.</span><span class="n">evalOpAwaitDefault</span><span class="p">(</span><span class="mi">10000</span><span class="p">);</span>

    <span class="c1">// Run Async Kompute operation on the parameters provided</span>
    <span class="n">mgr</span><span class="p">.</span><span class="n">evalOpAsyncDefault</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">OpAlgoBase</span><span class="o">&lt;&gt;&gt;</span><span class="p">(</span>
        <span class="p">{</span> <span class="n">tensor</span> <span class="p">},</span>
        <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">char</span><span class="o">&gt;</span><span class="p">(</span><span class="n">shader</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">shader</span><span class="p">.</span><span class="n">end</span><span class="p">()));</span>

    <span class="c1">// Here we can do other work</span>

    <span class="c1">// When we're ready we can wait</span>
    <span class="c1">// The default wait time is UINT64_MAX</span>
    <span class="n">mgr</span><span class="p">.</span><span class="n">evalOpAwaitDefault</span><span class="p">()</span>

    <span class="c1">// Sync the GPU memory back to the local tensor</span>
    <span class="c1">// We can still run synchronous jobs in our created sequence</span>
    <span class="n">mgr</span><span class="p">.</span><span class="n">evalOpDefault</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">OpTensorSyncLocal</span><span class="o">&gt;</span><span class="p">({</span> <span class="n">tensor</span> <span class="p">});</span>

    <span class="c1">// Prints the output: B: { 100000000, ... }</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">fmt</span><span class="o">::</span><span class="n">format</span><span class="p">(</span><span class="s">"B: {}"</span><span class="p">,</span>
        <span class="n">tensor</span><span class="p">.</span><span class="n">data</span><span class="p">())</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</td></tr></table></div>


<h3 id="parallel-operations">Parallel Operations<a class="headerlink" href="#parallel-operations" title="Permalink to this headline">¶</a></h3>
<p>Besides being able to submit asynchronous operations, you can also leverage the underlying GPU compute queues to process operations in parallel.</p>
<p>This will depend on your underlying graphics card, but for example in NVIDIA graphics cards the operations submitted across queues in one family are not parallelizable, but operations submitted across queueFamilies can be parallelizable.</p>
<p>Below we show how you can parallelize operations in an <a class="reference external" href="http://vulkan.gpuinfo.org/displayreport.php?id=9700#queuefamilies">NVIDIA 1650</a>, which has a <code class="docutils literal notranslate"><span class="pre">GRAPHICS+COMPUTE</span></code> family on <code class="docutils literal notranslate"><span class="pre">index</span> <span class="pre">0</span></code>, and <code class="docutils literal notranslate"><span class="pre">COMPUTE</span></code> family on <code class="docutils literal notranslate"><span class="pre">index</span> <span class="pre">2</span></code>.</p>
<p>Back to <a class="reference external" href="#simple-examples">examples list</a>.</p>
<div class="highlight-cpp notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="kt">int</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>

    <span class="c1">// In this case we select device 0, and for queues, one queue from familyIndex 0</span>
    <span class="c1">// and one queue from familyIndex 2</span>
    <span class="kt">uint32_t</span> <span class="n">deviceIndex</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="o">&gt;</span> <span class="n">familyIndices</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">};</span>

    <span class="c1">// We create a manager with device index, and queues by queue family index</span>
    <span class="n">kp</span><span class="o">::</span><span class="n">Manager</span> <span class="n">mgr</span><span class="p">(</span><span class="n">deviceIndex</span><span class="p">,</span> <span class="n">familyIndices</span><span class="p">);</span>

    <span class="c1">// We need to create explicit sequences with their respective queues</span>
    <span class="c1">// The second parameter is the index in the familyIndex array which is relative</span>
    <span class="c1">//      to the vector we created the manager with.</span>
    <span class="n">mgr</span><span class="p">.</span><span class="n">createManagedSequence</span><span class="p">(</span><span class="s">"queueOne"</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
    <span class="n">mgr</span><span class="p">.</span><span class="n">createManagedSequence</span><span class="p">(</span><span class="s">"queueTwo"</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>

    <span class="c1">// Creates tensor an initializes GPU memory (below we show more granularity)</span>
    <span class="k">auto</span> <span class="n">tensorA</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="p">(</span><span class="n">kp</span><span class="o">::</span><span class="n">Tensor</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)));</span>
    <span class="k">auto</span> <span class="n">tensorB</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="p">(</span><span class="n">kp</span><span class="o">::</span><span class="n">Tensor</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)));</span>

    <span class="c1">// We run the first step synchronously on the default sequence</span>
    <span class="n">mgr</span><span class="p">.</span><span class="n">evalOpDefault</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">OpTensorCreate</span><span class="o">&gt;</span><span class="p">({</span> <span class="n">tensorA</span><span class="p">,</span> <span class="n">tensorB</span> <span class="p">});</span>

    <span class="c1">// Define your shader as a string (using string literals for simplicity)</span>
    <span class="c1">// (You can also pass the raw compiled bytes, or even path to file)</span>
    <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">shader</span><span class="p">(</span><span class="sa">R</span><span class="s">"</span><span class="dl">(</span><span class="s"></span>
<span class="s">        #version 450</span>

<span class="s">        layout (local_size_x = 1) in;</span>

<span class="s">        layout(set = 0, binding = 0) buffer b { float pb[]; };</span>

<span class="s">        shared uint sharedTotal[1];</span>

<span class="s">        void main() {</span>
<span class="s">            uint index = gl_GlobalInvocationID.x;</span>

<span class="s">            sharedTotal[0] = 0;</span>

<span class="s">            // Iterating to simulate longer process</span>
<span class="s">            for (int i = 0; i &lt; 100000000; i++)</span>
<span class="s">            {</span>
<span class="s">                atomicAdd(sharedTotal[0], 1);</span>
<span class="s">            }</span>

<span class="s">            pb[index] = sharedTotal[0];</span>
<span class="s">        }</span>
<span class="s">    </span><span class="dl">)</span><span class="s">"</span><span class="p">);</span>

    <span class="c1">// Run the first parallel operation in the `queueOne` sequence</span>
    <span class="n">mgr</span><span class="p">.</span><span class="n">evalOpAsync</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">OpAlgoBase</span><span class="o">&lt;&gt;&gt;</span><span class="p">(</span>
        <span class="p">{</span> <span class="n">tensorA</span> <span class="p">},</span>
        <span class="s">"queueOne"</span><span class="p">,</span>
        <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">char</span><span class="o">&gt;</span><span class="p">(</span><span class="n">shader</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">shader</span><span class="p">.</span><span class="n">end</span><span class="p">()));</span>

    <span class="c1">// Run the second parallel operation in the `queueTwo` sequence</span>
    <span class="n">mgr</span><span class="p">.</span><span class="n">evalOpAsync</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">OpAlgoBase</span><span class="o">&lt;&gt;&gt;</span><span class="p">(</span>
        <span class="p">{</span> <span class="n">tensorB</span> <span class="p">},</span>
        <span class="s">"queueTwo"</span><span class="p">,</span>
        <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">char</span><span class="o">&gt;</span><span class="p">(</span><span class="n">shader</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">shader</span><span class="p">.</span><span class="n">end</span><span class="p">()));</span>

    <span class="c1">// Here we can do other work</span>

    <span class="c1">// We can now wait for the two parallel tasks to finish</span>
    <span class="n">mgr</span><span class="p">.</span><span class="n">evalOpAwait</span><span class="p">(</span><span class="s">"queueOne"</span><span class="p">)</span>
    <span class="n">mgr</span><span class="p">.</span><span class="n">evalOpAwait</span><span class="p">(</span><span class="s">"queueTwo"</span><span class="p">)</span>

    <span class="c1">// Sync the GPU memory back to the local tensor</span>
    <span class="n">mgr</span><span class="p">.</span><span class="n">evalOp</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">OpTensorSyncLocal</span><span class="o">&gt;</span><span class="p">({</span> <span class="n">tensorA</span><span class="p">,</span> <span class="n">tensorB</span> <span class="p">});</span>

    <span class="c1">// Prints the output: A: 100000000 B: 100000000</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">fmt</span><span class="o">::</span><span class="n">format</span><span class="p">(</span><span class="s">"A: {}, B: {}"</span><span class="p">,</span>
        <span class="n">tensorA</span><span class="p">.</span><span class="n">data</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">tensorB</span><span class="p">.</span><span class="n">data</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</td></tr></table></div>



<h2 id="your-custom-kompute-operation">Your Custom Kompute Operation<a class="headerlink" href="#your-custom-kompute-operation" title="Permalink to this headline">¶</a></h2>
<p>Build your own pre-compiled operations for domain specific workflows. Back to <a class="reference external" href="#simple-examples">examples list</a></p>
<p>We also provide tools that allow you to <a class="reference external" href="https://github.com/EthicalML/vulkan-kompute/blob/master/scripts/convert_shaders.py#L40">convert shaders into C++ headers</a>.</p>
<div class="highlight-cpp notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="k">template</span><span class="o">&lt;</span><span class="kt">uint32_t</span> <span class="n">tX</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="kt">uint32_t</span> <span class="n">tY</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="kt">uint32_t</span> <span class="n">tZ</span> <span class="o">=</span> <span class="mi">0</span><span class="o">&gt;</span>
<span class="k">class</span> <span class="nc">OpMyCustom</span> <span class="o">:</span> <span class="k">public</span> <span class="n">OpAlgoBase</span><span class="o">&lt;</span><span class="n">tX</span><span class="p">,</span> <span class="n">tY</span><span class="p">,</span> <span class="n">tZ</span><span class="o">&gt;</span>
<span class="p">{</span>
  <span class="k">public</span><span class="o">:</span>
    <span class="n">OpMyCustom</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">vk</span><span class="o">::</span><span class="n">PhysicalDevice</span><span class="o">&gt;</span> <span class="n">physicalDevice</span><span class="p">,</span>
           <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">vk</span><span class="o">::</span><span class="n">Device</span><span class="o">&gt;</span> <span class="n">device</span><span class="p">,</span>
           <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">vk</span><span class="o">::</span><span class="n">CommandBuffer</span><span class="o">&gt;</span> <span class="n">commandBuffer</span><span class="p">,</span>
           <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;&gt;</span> <span class="n">tensors</span><span class="p">)</span>
      <span class="o">:</span> <span class="n">OpAlgoBase</span><span class="o">&lt;</span><span class="n">tX</span><span class="p">,</span> <span class="n">tY</span><span class="p">,</span> <span class="n">tZ</span><span class="o">&gt;</span><span class="p">(</span><span class="n">physicalDevice</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">commandBuffer</span><span class="p">,</span> <span class="n">tensors</span><span class="p">,</span> <span class="s">""</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="c1">// Perform your custom steps such as reading from a shader file</span>
        <span class="k">this</span><span class="o">-&gt;</span><span class="n">mShaderFilePath</span> <span class="o">=</span> <span class="s">"shaders/glsl/opmult.comp"</span><span class="p">;</span>
    <span class="p">}</span>
<span class="p">}</span>


<span class="kt">int</span> <span class="n">main</span><span class="p">()</span> <span class="p">{</span>

    <span class="n">kp</span><span class="o">::</span><span class="n">Manager</span> <span class="n">mgr</span><span class="p">;</span> <span class="c1">// Automatically selects Device 0</span>

    <span class="c1">// Create 3 tensors of default type float</span>
    <span class="k">auto</span> <span class="n">tensorLhs</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="p">(</span><span class="n">kp</span><span class="o">::</span><span class="n">Tensor</span><span class="p">({</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span> <span class="p">}));</span>
    <span class="k">auto</span> <span class="n">tensorRhs</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="p">(</span><span class="n">kp</span><span class="o">::</span><span class="n">Tensor</span><span class="p">({</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">,</span> <span class="mf">6.</span> <span class="p">}));</span>
    <span class="k">auto</span> <span class="n">tensorOut</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="p">(</span><span class="n">kp</span><span class="o">::</span><span class="n">Tensor</span><span class="p">({</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span> <span class="p">}));</span>

    <span class="c1">// Create tensors data explicitly in GPU with an operation</span>
    <span class="n">mgr</span><span class="p">.</span><span class="n">evalOpDefault</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">OpTensorCreate</span><span class="o">&gt;</span><span class="p">({</span> <span class="n">tensorLhs</span><span class="p">,</span> <span class="n">tensorRhs</span><span class="p">,</span> <span class="n">tensorOut</span> <span class="p">});</span>

    <span class="c1">// Run Kompute operation on the parameters provided with dispatch layout</span>
    <span class="n">mgr</span><span class="p">.</span><span class="n">evalOpDefault</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">OpMyCustom</span><span class="o">&lt;</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="o">&gt;&gt;</span><span class="p">(</span>
        <span class="p">{</span> <span class="n">tensorLhs</span><span class="p">,</span> <span class="n">tensorRhs</span><span class="p">,</span> <span class="n">tensorOut</span> <span class="p">});</span>

    <span class="c1">// Prints the output which is { 0, 4, 12 }</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">fmt</span><span class="o">::</span><span class="n">format</span><span class="p">(</span><span class="s">"Output: {}"</span><span class="p">,</span> <span class="n">tensorOutput</span><span class="p">.</span><span class="n">data</span><span class="p">())</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</td></tr></table></div>


<h2 id="logistic-regression-example">Logistic Regression Example<a class="headerlink" href="#logistic-regression-example" title="Permalink to this headline">¶</a></h2>
<p>Logistic regression is oftens seen as the hello world in machine learning so we will be using it for our examples. Back to <a class="reference external" href="#simple-examples">examples list</a>.</p>
<a class="reference internal image-reference" href="../_images/logistic-regression.jpg"><img alt="../_images/logistic-regression.jpg" src="../_images/logistic-regression.jpg" style="width: 300px;"/></a>
<p>In summary, we have:</p>
<ul class="simple">
<li><p>Vector <code class="docutils literal notranslate"><span class="pre">X</span></code> with input data (with a pair of inputs <code class="docutils literal notranslate"><span class="pre">Xi</span></code> and <code class="docutils literal notranslate"><span class="pre">Xj</span></code>)</p></li>
<li><p>Output <code class="docutils literal notranslate"><span class="pre">Y</span></code> with expected predictions</p></li>
</ul>
<p>With this we will:</p>
<ul class="simple">
<li><p>Optimize the function simplified as <code class="docutils literal notranslate"><span class="pre">Y</span> <span class="pre">=</span> <span class="pre">WX</span> <span class="pre">+</span> <span class="pre">b</span></code></p></li>
<li><p>We’ll want our program to learn the parameters <code class="docutils literal notranslate"><span class="pre">W</span></code> and <code class="docutils literal notranslate"><span class="pre">b</span></code></p></li>
</ul>
<p>Converting to Kompute Terminology</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>We will have to convert this into Kompute terminology.

First specifically around the inputs, we will be using the following:

* Two vertors for the variable `X`, vector `Xi` and `Xj`
* One vector `Y` for the true predictions
* A vector `W` containing the two input weight values to use for inference
* A vector `B` containing a single input parameter for `b`

.. code-block:: cpp
    :linenos:

    std::vector&lt;float&gt; wInVec = { 0.001, 0.001 };
    std::vector&lt;float&gt; bInVec = { 0 };

    std::shared_ptr&lt;kp::Tensor&gt; xI{ new kp::Tensor({ 0, 1, 1, 1, 1 })};
    std::shared_ptr&lt;kp::Tensor&gt; xJ{ new kp::Tensor({ 0, 0, 0, 1, 1 })};

    std::shared_ptr&lt;kp::Tensor&gt; y{ new kp::Tensor({ 0, 0, 0, 1, 1 })};

    std::shared_ptr&lt;kp::Tensor&gt; wIn{
        new kp::Tensor(wInVec, kp::Tensor::TensorTypes::eStaging)};

    std::shared_ptr&lt;kp::Tensor&gt; bIn{
        new kp::Tensor(bInVec, kp::Tensor::TensorTypes::eStaging)};


We will have the following output vectors:

* Two output vectors `Wi` and `Wj` to store all the deltas to perform gradient descent on W
* One output vector `Bout` to store all the deltas to perform gradient descent on B

.. code-block:: cpp
    :linenos:

    std::shared_ptr&lt;kp::Tensor&gt; wOutI{ new kp::Tensor({ 0, 0, 0, 0, 0 })};
    std::shared_ptr&lt;kp::Tensor&gt; wOutJ{ new kp::Tensor({ 0, 0, 0, 0, 0 })};

    std::shared_ptr&lt;kp::Tensor&gt; bOut{ new kp::Tensor({ 0, 0, 0, 0, 0 })};


For simplicity we will store all the tensors inside a params variable:

.. code-block:: cpp
    :linenos:

    std::vector&lt;std::shared_ptr&lt;kp::Tensor&gt;&gt; params =
        {xI, xJ, y, wIn, wOutI, wOutJ, bIn, bOut};


Now that we have the inputs and outputs we will be able to use them in the processing. The workflow we will be using is the following:

1. Create a Sequence to record and submit GPU commands
2. Submit OpCreateTensor to create all the tensors
3. Record the OpAlgo with the Logistic Regression shader
4. Loop across number of iterations:
   4-a. Submit algo operation on LR shader
   4-b. Re-calculate weights from loss
5. Print output weights and bias

1. Create a sequence to record and submit GPU commands
</pre></div>
</div>
<div class="highlight-cpp notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3
4
5
6</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="n">kp</span><span class="o">::</span><span class="n">Manager</span> <span class="n">mgr</span><span class="p">;</span>

<span class="k">if</span> <span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">Sequence</span><span class="o">&gt;</span> <span class="n">sq</span> <span class="o">=</span>
        <span class="n">mgr</span><span class="p">.</span><span class="n">getOrCreateManagedSequence</span><span class="p">(</span><span class="s">"createTensors"</span><span class="p">).</span><span class="n">lock</span><span class="p">())</span>
<span class="p">{</span>
    <span class="c1">// ...</span>
</pre></div>
</td></tr></table></div>
<ol class="arabic simple">
<li><p>Submit OpCreateTensor to create all the tensors
<span class="raw-html-m2r"><del>~</del></span><span class="raw-html-m2r"><del>~</del></span><span class="raw-html-m2r"><del>~</del></span><span class="raw-html-m2r"><del>~</del></span>~~</p></li>
</ol>
<div class="highlight-cpp notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3
4
5
6
7
8
9</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="c1">// ... continuing from codeblock above</span>

    <span class="n">sq</span><span class="o">-&gt;</span><span class="n">begin</span><span class="p">();</span>

    <span class="n">sq</span><span class="o">-&gt;</span><span class="n">record</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">OpCreateTensor</span><span class="o">&gt;</span><span class="p">(</span><span class="n">params</span><span class="p">);</span>

    <span class="n">sq</span><span class="o">-&gt;</span><span class="n">end</span><span class="p">();</span>
    <span class="n">sq</span><span class="o">-&gt;</span><span class="n">eval</span><span class="p">();</span>
</pre></div>
</td></tr></table></div>
<ol class="arabic simple">
<li><p>Record the OpAlgo with the Logistic Regression shader
<span class="raw-html-m2r"><del>~</del></span><span class="raw-html-m2r"><del>~</del></span><span class="raw-html-m2r"><del>~</del></span><span class="raw-html-m2r"><del>~</del></span>~~</p></li>
</ol>
<p>Once we re-record, all the instructions that were recorded previously are cleared.</p>
<p>Because of this we can record now the new commands which will consist of the following:</p>
<ol class="arabic simple">
<li><p>Copy the tensor data from local to device</p></li>
<li><p>Run the logistic regression shader</p></li>
<li><p>Copy the output data</p></li>
</ol>
<div class="highlight-cpp notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="c1">// ... continuing from codeblock above</span>

    <span class="n">sq</span><span class="o">-&gt;</span><span class="n">begin</span><span class="p">();</span>

    <span class="n">sq</span><span class="o">-&gt;</span><span class="n">record</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">OpTensorSyncDevice</span><span class="o">&gt;</span><span class="p">({</span><span class="n">wIn</span><span class="p">,</span> <span class="n">bIn</span><span class="p">});</span>

    <span class="n">sq</span><span class="o">-&gt;</span><span class="n">record</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">OpAlgoBase</span><span class="o">&lt;&gt;&gt;</span><span class="p">(</span>
            <span class="n">params</span><span class="p">,</span>
            <span class="nb">false</span><span class="p">,</span> <span class="c1">// Whether to copy output from device</span>
            <span class="s">"test/shaders/glsl/test_logistic_regression.comp"</span><span class="p">);</span>

    <span class="n">sq</span><span class="o">-&gt;</span><span class="n">record</span><span class="o">&lt;</span><span class="n">kp</span><span class="o">::</span><span class="n">OpTensorSyncLocal</span><span class="o">&gt;</span><span class="p">({</span><span class="n">wOutI</span><span class="p">,</span> <span class="n">wOutJ</span><span class="p">,</span> <span class="n">bOut</span><span class="p">});</span>

    <span class="n">sq</span><span class="o">-&gt;</span><span class="n">end</span><span class="p">();</span>
</pre></div>
</td></tr></table></div>
<ol class="arabic simple">
<li><p>Loop across number of iterations + 4-a. Submit algo operation on LR shader
<span class="raw-html-m2r"><del>~</del></span><span class="raw-html-m2r"><del>~</del></span><span class="raw-html-m2r"><del>~</del></span><span class="raw-html-m2r"><del>~</del></span>~~</p></li>
</ol>
<div class="highlight-cpp notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3
4
5
6
7
8
9</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="c1">// ... continuing from codeblock above</span>

    <span class="kt">uint32_t</span> <span class="n">ITERATIONS</span> <span class="o">=</span> <span class="mi">100</span><span class="p">;</span>

    <span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">ITERATIONS</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="c1">// Run evaluation which passes data through shader once</span>
        <span class="n">sq</span><span class="o">-&gt;</span><span class="n">eval</span><span class="p">();</span>
</pre></div>
</td></tr></table></div>
<p>4-b. Re-calculate weights from loss</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Once</span> <span class="n">the</span> <span class="n">shader</span> <span class="n">code</span> <span class="ow">is</span> <span class="n">executed</span><span class="p">,</span> <span class="n">we</span> <span class="n">are</span> <span class="n">able</span> <span class="n">to</span> <span class="n">use</span> <span class="n">the</span> <span class="n">outputs</span> <span class="kn">from</span> <span class="nn">the</span> <span class="n">shader</span> <span class="n">calculation</span><span class="o">.</span>

<span class="n">In</span> <span class="n">this</span> <span class="n">case</span> <span class="n">we</span> <span class="n">want</span> <span class="n">to</span> <span class="n">basically</span> <span class="n">add</span> <span class="nb">all</span> <span class="n">the</span> <span class="n">calculated</span> <span class="n">weights</span> <span class="ow">and</span> <span class="n">bias</span> <span class="kn">from</span> <span class="nn">the</span> <span class="n">back</span><span class="o">-</span><span class="n">prop</span> <span class="n">step</span><span class="o">.</span>

<span class="o">..</span> <span class="n">code</span><span class="o">-</span><span class="n">block</span><span class="p">::</span> <span class="n">cpp</span>
    <span class="p">:</span><span class="n">linenos</span><span class="p">:</span>

    <span class="p">{</span>
        <span class="o">//</span> <span class="o">...</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">ITERATIONS</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
        <span class="p">{</span>
            <span class="o">//</span> <span class="o">...</span> <span class="n">continuing</span> <span class="kn">from</span> <span class="nn">codeblock</span> <span class="n">above</span>

            <span class="o">//</span> <span class="n">Run</span> <span class="n">evaluation</span> <span class="n">which</span> <span class="n">passes</span> <span class="n">data</span> <span class="n">through</span> <span class="n">shader</span> <span class="n">once</span>
            <span class="n">sq</span><span class="o">-&gt;</span><span class="nb">eval</span><span class="p">();</span>

            <span class="o">//</span> <span class="n">Subtract</span> <span class="n">the</span> <span class="n">resulting</span> <span class="n">weights</span> <span class="ow">and</span> <span class="n">biases</span>
            <span class="k">for</span><span class="p">(</span><span class="n">size_t</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">bOut</span><span class="o">-&gt;</span><span class="n">size</span><span class="p">();</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                <span class="n">wInVec</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-=</span> <span class="n">wOutI</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">()[</span><span class="n">j</span><span class="p">];</span>
                <span class="n">wInVec</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-=</span> <span class="n">wOutJ</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">()[</span><span class="n">j</span><span class="p">];</span>
                <span class="n">bInVec</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-=</span> <span class="n">bOut</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">()[</span><span class="n">j</span><span class="p">];</span>
            <span class="p">}</span>
            <span class="o">//</span> <span class="n">Set</span> <span class="n">the</span> <span class="n">data</span> <span class="k">for</span> <span class="n">the</span> <span class="n">GPU</span> <span class="n">to</span> <span class="n">use</span> <span class="ow">in</span> <span class="n">the</span> <span class="nb">next</span> <span class="n">iteration</span>
            <span class="n">wIn</span><span class="o">-&gt;</span><span class="n">mapDataIntoHostMemory</span><span class="p">();</span>
            <span class="n">bIn</span><span class="o">-&gt;</span><span class="n">mapDataIntoHostMemory</span><span class="p">();</span>
        <span class="p">}</span>

<span class="mf">5.</span> <span class="n">Print</span> <span class="n">output</span> <span class="n">weights</span> <span class="ow">and</span> <span class="n">bias</span>
</pre></div>
</div>
<div class="highlight-cpp notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"Weight i: "</span> <span class="o">&lt;&lt;</span> <span class="n">wIn</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"Weight j: "</span> <span class="o">&lt;&lt;</span> <span class="n">wIn</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"Bias: "</span> <span class="o">&lt;&lt;</span> <span class="n">bIn</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
</pre></div>
</td></tr></table></div>
<p>Finally you can see the shader used for the logistic regression usecase below:</p>
<div class="highlight-cpp notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="cp">#version 450</span>

<span class="n">layout</span> <span class="p">(</span><span class="n">constant_id</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="k">const</span> <span class="n">uint</span> <span class="n">M</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>

<span class="n">layout</span> <span class="p">(</span><span class="n">local_size_x</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="n">in</span><span class="p">;</span>

<span class="n">layout</span><span class="p">(</span><span class="n">set</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">binding</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="n">buffer</span> <span class="n">bxi</span> <span class="p">{</span> <span class="kt">float</span> <span class="n">xi</span><span class="p">[];</span> <span class="p">};</span>
<span class="n">layout</span><span class="p">(</span><span class="n">set</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">binding</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="n">buffer</span> <span class="n">bxj</span> <span class="p">{</span> <span class="kt">float</span> <span class="n">xj</span><span class="p">[];</span> <span class="p">};</span>
<span class="n">layout</span><span class="p">(</span><span class="n">set</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">binding</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span> <span class="n">buffer</span> <span class="n">by</span> <span class="p">{</span> <span class="kt">float</span> <span class="n">y</span><span class="p">[];</span> <span class="p">};</span>
<span class="n">layout</span><span class="p">(</span><span class="n">set</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">binding</span> <span class="o">=</span> <span class="mi">3</span><span class="p">)</span> <span class="n">buffer</span> <span class="n">bwin</span> <span class="p">{</span> <span class="kt">float</span> <span class="n">win</span><span class="p">[];</span> <span class="p">};</span>
<span class="n">layout</span><span class="p">(</span><span class="n">set</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">binding</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span> <span class="n">buffer</span> <span class="n">bwouti</span> <span class="p">{</span> <span class="kt">float</span> <span class="n">wouti</span><span class="p">[];</span> <span class="p">};</span>
<span class="n">layout</span><span class="p">(</span><span class="n">set</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">binding</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span> <span class="n">buffer</span> <span class="n">bwoutj</span> <span class="p">{</span> <span class="kt">float</span> <span class="n">woutj</span><span class="p">[];</span> <span class="p">};</span>
<span class="n">layout</span><span class="p">(</span><span class="n">set</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">binding</span> <span class="o">=</span> <span class="mi">6</span><span class="p">)</span> <span class="n">buffer</span> <span class="n">bbin</span> <span class="p">{</span> <span class="kt">float</span> <span class="n">bin</span><span class="p">[];</span> <span class="p">};</span>
<span class="n">layout</span><span class="p">(</span><span class="n">set</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">binding</span> <span class="o">=</span> <span class="mi">7</span><span class="p">)</span> <span class="n">buffer</span> <span class="n">bbout</span> <span class="p">{</span> <span class="kt">float</span> <span class="n">bout</span><span class="p">[];</span> <span class="p">};</span>

<span class="kt">float</span> <span class="n">learningRate</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">;</span>
<span class="kt">float</span> <span class="n">m</span> <span class="o">=</span> <span class="kt">float</span><span class="p">(</span><span class="n">M</span><span class="p">);</span>

<span class="kt">float</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="kt">float</span> <span class="n">z</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">return</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">));</span>
<span class="p">}</span>

<span class="kt">float</span> <span class="nf">inference</span><span class="p">(</span><span class="n">vec2</span> <span class="n">x</span><span class="p">,</span> <span class="n">vec2</span> <span class="n">w</span><span class="p">,</span> <span class="kt">float</span> <span class="n">b</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">float</span> <span class="n">z</span> <span class="o">=</span> <span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="p">;</span>
    <span class="kt">float</span> <span class="n">yHat</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">);</span>
    <span class="k">return</span> <span class="n">yHat</span><span class="p">;</span>
<span class="p">}</span>

<span class="kt">float</span> <span class="nf">calculateLoss</span><span class="p">(</span><span class="kt">float</span> <span class="n">yHat</span><span class="p">,</span> <span class="kt">float</span> <span class="n">y</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">return</span> <span class="o">-</span><span class="p">(</span><span class="n">y</span> <span class="o">*</span> <span class="n">log</span><span class="p">(</span><span class="n">yHat</span><span class="p">)</span>  <span class="o">+</span>  <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">log</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">yHat</span><span class="p">));</span>
<span class="p">}</span>

<span class="kt">void</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
    <span class="n">uint</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">gl_GlobalInvocationID</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>

    <span class="n">vec2</span> <span class="n">wCurr</span> <span class="o">=</span> <span class="n">vec2</span><span class="p">(</span><span class="n">win</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">win</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span>
    <span class="kt">float</span> <span class="n">bCurr</span> <span class="o">=</span> <span class="n">bin</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>

    <span class="n">vec2</span> <span class="n">xCurr</span> <span class="o">=</span> <span class="n">vec2</span><span class="p">(</span><span class="n">xi</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">xj</span><span class="p">[</span><span class="n">idx</span><span class="p">]);</span>
    <span class="kt">float</span> <span class="n">yCurr</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">idx</span><span class="p">];</span>

    <span class="kt">float</span> <span class="n">yHat</span> <span class="o">=</span> <span class="n">inference</span><span class="p">(</span><span class="n">xCurr</span><span class="p">,</span> <span class="n">wCurr</span><span class="p">,</span> <span class="n">bCurr</span><span class="p">);</span>
    <span class="kt">float</span> <span class="n">loss</span> <span class="o">=</span> <span class="n">calculateLoss</span><span class="p">(</span><span class="n">yHat</span><span class="p">,</span> <span class="n">yCurr</span><span class="p">);</span>

    <span class="kt">float</span> <span class="n">dZ</span> <span class="o">=</span> <span class="n">yHat</span> <span class="o">-</span> <span class="n">yCurr</span><span class="p">;</span>
    <span class="n">vec2</span> <span class="n">dW</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">/</span> <span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">xCurr</span> <span class="o">*</span> <span class="n">dZ</span><span class="p">;</span>
    <span class="kt">float</span> <span class="n">dB</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">/</span> <span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">dZ</span><span class="p">;</span>
    <span class="n">wouti</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">learningRate</span> <span class="o">*</span> <span class="n">dW</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
    <span class="n">woutj</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">learningRate</span> <span class="o">*</span> <span class="n">dW</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
    <span class="n">bout</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">learningRate</span> <span class="o">*</span> <span class="n">dB</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</td></tr></table></div>




          </article>
        </div>
      </div>
    </main>
  </div>
  <footer class="md-footer">
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
          
            <a href="../index.html" title="Kompute Docs Home"
               class="md-flex md-footer-nav__link md-footer-nav__link--prev"
               rel="prev">
              <div class="md-flex__cell md-flex__cell--shrink">
                <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
              </div>
              <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
                <span class="md-flex__ellipsis">
                  <span
                      class="md-footer-nav__direction"> Previous </span> Kompute Docs Home </span>
              </div>
            </a>
          
          
            <a href="python-package.html" title="Python Package Overview"
               class="md-flex md-footer-nav__link md-footer-nav__link--next"
               rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title"><span
                class="md-flex__ellipsis"> <span
                class="md-footer-nav__direction"> Next </span> Python Package Overview </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink"><i
                class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          
        </a>
        
      </nav>
    </div>
    <div class="md-footer-meta md-typeset">
      <div class="md-footer-meta__inner md-grid">
        <div class="md-footer-copyright">
          <div class="md-footer-copyright__highlight">
              &#169; Copyright 2020, The Institute for Ethical AI &amp; Machine Learning.
              
          </div>
            Created using
            <a href="http://www.sphinx-doc.org/">Sphinx</a> 3.2.1.
             and
            <a href="https://github.com/bashtage/sphinx-material/">Material for
              Sphinx</a>
        </div>
      </div>
    </div>
  </footer>
  <script src="../_static/javascripts/application.js"></script>
  <script>app.initialize({version: "1.0.4", url: {base: ".."}})</script>
  </body>
</html>